---
title: "Optimize"
description: "Prompt versions, tests, and no-code deploys"
icon: "wand-magic-sparkles"
---

## The prompt iteration loop

Once you know which prompt performs better from your [evaluations](/documentation/getting-started/quickstart/evaluate-workflow), the next step is to iterate. Respan's prompt management gives you version control for every prompt — edit, commit new versions, test them with experiments, and deploy the winner to production. No code changes required.

The workflow:

1. **Manage prompt versions** — edit and commit new versions on the platform
2. **Test with experiments** — run the same dataset against different versions to compare
3. **Deploy the best version** — push the winner to production with one click

---

## 1. Manage prompt versions

Every prompt on Respan has full version history. Instead of changing prompts in your codebase, edit them on the [Prompts page](https://platform.respan.ai/platform/prompts) — change the prompt text, model, temperature, max tokens, or any other configuration, then commit a new version.

### Use variables

Use `{{variable_name}}` in your prompts to add dynamic content. Variables let you reuse the same prompt template across different inputs — pass values at runtime from your code or fill them in from testset columns when running experiments.

```
You are a helpful customer support agent for {{company_name}}.
The customer {{customer_name}} is asking about {{issue_type}}.
Please provide a clear and helpful response.
```

<Note>
Variable names must use underscores, not spaces — `{{task_description}}` not `{{task description}}`.
</Note>

<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/get-started/prompt-variables.png" alt="Prompt variables" />

### Edit and commit

<Steps>
<Step title="Edit the prompt">
Open your prompt in the **Editor** tab. Update the prompt content, variables, model, temperature, max tokens, top P, or any other parameter.

<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/get-started/configure-prompt.png" alt="Configure prompt" />
</Step>

<Step title="Commit a new version">
Click **Commit** and write a commit message describing what changed. This saves a new version without affecting production.

<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/get-started/prompt-commit.png" alt="Commit prompt version" />
</Step>
</Steps>

Each version is saved and accessible — you can compare any two versions, roll back, or test them side by side in experiments.

<Card title="Manage prompts" icon="code-branch" href="/documentation/features/prompt-management/manage-prompts">
  Create, configure, and version prompt templates
</Card>

---

## 2. Test versions with experiments

Don't guess which version is better — test it. Run the same dataset against multiple prompt versions in an [experiment](/documentation/getting-started/quickstart/evaluate-workflow) to compare outputs and evaluator scores side by side.

<Steps>
<Step title="Create an experiment">
Go to [Experiments](https://platform.respan.ai/platform/experiments) and click **New experiment**. Select your dataset.

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/documentation/products/experiments_v2/select_dataset.png" alt="Select dataset" />
</Frame>
</Step>

<Step title="Select prompt versions to compare">
Choose the prompt and select the versions you want to test. Each version may have different prompt text, model, temperature, max tokens, or other configurations.

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/documentation/products/experiments_v2/select_task.png" alt="Select prompt versions" />
</Frame>
</Step>

<Step title="Run and compare scores">
Run the experiment and apply evaluators. Compare scores across versions to see which one performs best.

<Frame>
<video
  controls
  className="w-full aspect-video"
  src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/experiments/evals-result.mp4"
></video>
</Frame>
</Step>
</Steps>

Repeat this cycle — edit prompt, commit version, run experiment — until you're confident in the result.

---

## 3. Deploy to production

Once you've found the best version, deploy it to production. Go to the **Deployments** tab and click **Deploy**. The deployed version goes live immediately — all API calls using this prompt will start using the new version.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/prompts-deploy.jpg" alt="Deploy prompt" />
</Frame>

<Warning>
Deploying a prompt takes effect immediately. All API calls using this prompt will start using the deployed version right away.
</Warning>

Your code doesn't need to change — it references the prompt by ID, and Respan serves the deployed version automatically:

<CodeGroup>
```python Python
from openai import OpenAI

client = OpenAI(
    base_url="https://api.respan.ai/api/",
    api_key="YOUR_RESPAN_API_KEY",
)

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "placeholder"}],
    extra_body={
        "prompt": {
            "prompt_id": "042f5f",
            "variables": {
                "task_description": "Square a number",
                "specific_library": "math"
            },
            "override": True
        }
    }
)
```
```typescript TypeScript
const response = await client.chat.completions.create({
    messages: [{ role: "user", content: "placeholder" }],
    model: "gpt-4o-mini",
    // @ts-expect-error
    prompt: {
        prompt_id: "042f5f",
        variables: {
            task_description: "Square a number",
            specific_library: "math"
        },
        override: true
    }
});
```
</CodeGroup>

<Card title="Use prompt in code" icon="rocket" href="/documentation/features/prompt-management/use-prompt">
  Deploy prompts to your codebase via gateway or logging API
</Card>
