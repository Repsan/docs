---
title: "Deploy"
description: "Route traffic with caching, failover, and controls"
icon: "rocket"
---

## Separate prompts from your codebase

Once you have your optimized prompts ready from [evaluation](/documentation/getting-started/quickstart/evaluate-workflow) and [prompt optimization](/documentation/getting-started/quickstart/optimize-workflow), you can deploy them through the Respan AI gateway. Instead of hardcoding prompts and model configurations in your application, the gateway lets you manage everything from the platform — update prompts, swap models, add fallbacks and retries, all without redeploying your code.

---

## 1. Serve prompts from the platform

Reference your prompt by ID in code. The gateway serves the deployed version automatically — when you deploy a new version on the platform, your application picks it up immediately with no code changes.

<CodeGroup>
```python Python
from openai import OpenAI

client = OpenAI(
    base_url="https://api.respan.ai/api/",
    api_key="YOUR_RESPAN_API_KEY",
)

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "placeholder"}],
    extra_body={
        "prompt": {
            "prompt_id": "042f5f",
            "variables": {
                "customer_name": "John",
                "issue_type": "billing"
            },
            "override": True
        }
    }
)
```
```typescript TypeScript
import { OpenAI } from "openai";

const client = new OpenAI({
    baseURL: "https://api.respan.ai/api",
    apiKey: "YOUR_RESPAN_API_KEY",
});

const response = await client.chat.completions.create({
    messages: [{ role: "user", content: "placeholder" }],
    model: "gpt-4o-mini",
    // @ts-expect-error
    prompt: {
        prompt_id: "042f5f",
        variables: {
            customer_name: "John",
            issue_type: "billing"
        },
        override: true
    }
});
```
</CodeGroup>

With `override: True`, the gateway uses the prompt's saved model, messages, temperature, and all other configurations — the `model` and `messages` fields in your code are ignored.

<Card title="Use prompt in code" icon="rocket" href="/documentation/features/prompt-management/use-prompt">
  Full guide on deploying prompts via gateway
</Card>

---

## 2. Add fallbacks and retries

LLM providers have outages. The gateway handles this automatically — configure fallback models so if your primary model fails, the request is retried on an alternative.

```python
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "fallback_models": ["claude-3-5-sonnet-20241022", "gemini-1.5-pro"],
        "retries": 2
    }
)
```

If `gpt-4o` fails, the gateway automatically tries `claude-3-5-sonnet`, then `gemini-1.5-pro`. Your users never see the error.

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/llm-proxy/fallback.png" alt="Fallback models" />
</Frame>

---

## 3. Load balance across models

Distribute traffic across multiple models or deployments to avoid rate limits and optimize cost. Set up load balancing groups on the [Load balancing page](https://platform.respan.ai/platform/api/load-balance) or pass them in code.

```python
response = client.chat.completions.create(
    messages=[{"role": "user", "content": "Hello"}],
    model="",  # leave empty when using load balancing
    extra_body={
        "load_balance_group": {
            "group_id": "YOUR_GROUP_ID"
        }
    }
)
```

Configure weights per model to control how traffic is distributed based on rate limits and your preferences.

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/llm-proxy/load-balancing.jpg" alt="Load balancing" />
</Frame>

---

## 4. Enable caching

Reduce costs and latency by caching identical requests. When the same input is sent again, the gateway returns the cached response instantly.

```python
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is 2+2?"}],
    extra_body={
        "cache_enabled": True,
        "cache_ttl": 86400  # cache for 24 hours
    }
)
```

<Card title="Advanced configuration" icon="gear" href="/documentation/features/gateway/advanced-configuration">
  Caching, rate limits, function calling, and more
</Card>

---

## 5. Access 250+ models

The gateway supports 250+ models through a single API — OpenAI, Anthropic, Google, Azure, and more. Switch models by changing the `model` parameter, no SDK changes needed.

<CardGroup cols={2}>
<Card href="/integrations/gateway/openai/openai-sdk">
  <img className="block dark:hidden" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/openai_v0.png" alt="OpenAI" style={{
    pointerEvents: 'none'
  }} />
  <img className="hidden dark:block" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/openai_v0_black.png" alt="OpenAI" style={{
    pointerEvents: 'none'
  }} />
</Card>
<Card href="/integrations/gateway/langchain">
  <img className="block dark:hidden" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/langchain_v0.png" alt="LangChain" style={{
    pointerEvents: 'none'
  }} />
  <img className="hidden dark:block" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/langchain_v0_black.png" alt="LangChain" style={{
    pointerEvents: 'none'
  }} />
</Card>
<Card href="/integrations/gateway/vercel">
  <img className="block dark:hidden" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/vercel_v0.png" alt="Vercel AI SDK" style={{
    pointerEvents: 'none'
  }} />
  <img className="hidden dark:block" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/vercel_v0_black.png" alt="Vercel AI SDK" style={{
    pointerEvents: 'none'
  }} />
</Card>
<Card href="/integrations/gateway/anthropic">
  <img className="block dark:hidden" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/anthropic_v0.png" alt="Anthropic" style={{
    pointerEvents: 'none'
  }} />
  <img className="hidden dark:block" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/anthropic_v0_black.png" alt="Anthropic" style={{
    pointerEvents: 'none'
  }} />
</Card>
</CardGroup>

<Note>
The gateway adds **50–150ms** of latency. If your product has strict latency requirements, consider using [tracing](/documentation/getting-started/quickstart/tracing) for observability instead of routing through the gateway.
</Note>
