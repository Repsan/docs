---
title: "Monitor"
description: "Dashboards, alerts, online evals, and workflows"
icon: "chart-line"
---

## You have observability — now use it

Once you've set up [tracing](/documentation/getting-started/quickstart/tracing) or [logging](/documentation/getting-started/quickstart/logging), your data is flowing into Respan. Now you need a workflow to stay on top of your AI system — catch issues early, debug fast, and add guardrails to prevent problems before they reach users.

---

## 1. Check metrics daily

The [Dashboard](https://platform.respan.ai/platform/dashboard) gives you a high-level view of how your AI product is performing. Check it regularly to make sure things are running smoothly — total requests, error rates, latency, cost, and token usage across all your models.

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/documentation/products/dashboard/overview/dashboard.png" />
</Frame>

Use this to spot trends and anomalies: a sudden spike in errors, a model getting slower, or costs creeping up unexpectedly.

<Card title="Metrics" icon="chart-line" href="/documentation/features/monitoring/metrics">
  Track requests, tokens, latency, cost, and errors
</Card>

---

## 2. Debug with logs and traces

When something goes wrong — a user reports a bad response, or you see an error spike on the dashboard — drill into individual [logs](https://platform.respan.ai/platform/requests) and [traces](https://platform.respan.ai/platform/traces) to understand exactly what happened.

- **Logs** show you the full request/response for each LLM call — input, output, model, latency, cost, and status
- **Traces** show the full execution tree of multi-step workflows — which step failed, where the latency is, and how agents and tools interacted

<CardGroup cols={2}>
  <Card title="Logs" icon="file-lines" href="/documentation/features/tracing/logs/quickstart">
    Inspect individual LLM calls
  </Card>
  <Card title="Traces" icon="sitemap" href="/documentation/features/tracing/traces/quickstart">
    Debug multi-step agent workflows
  </Card>
</CardGroup>

---

## 3. Save views to skip repetitive filtering

If you find yourself applying the same filters repeatedly — like filtering for production errors, a specific model, or a particular user — save them as a **view**. Views are reusable filter configurations you can access with one click instead of reconfiguring every time.

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/documentation/products/dashboard/views/go_to_view_page_v0.png" />
</Frame>

Examples of useful views:
- **Production errors** — `status: error` + `env: production`
- **High-cost requests** — `cost > $0.10`
- **Specific user sessions** — `customer_identifier: user_123`

<Card title="Views" icon="bookmark" href="/documentation/features/monitoring/views">
  Save and reuse filter configurations
</Card>

---

## 4. Set up notifications

Don't wait until you check the dashboard to find out something is wrong. Set up **alerts** to get notified by email when issues are detected — LLM outages, error spikes, or warning conditions on your logs.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/observability/alerts.png"/>
</Frame>

<CardGroup cols={2}>
  <Card title="Alerts & warnings" icon="bell" href="/documentation/features/monitoring/notifications/subscribe_alerts">
    Get notified when issues are detected
  </Card>
  <Card title="Webhooks" icon="webhook" href="/documentation/features/monitoring/notifications/webhooks">
    Send events to your own systems
  </Card>
</CardGroup>

---

## 5. Add guardrails with automations

Automations let you add guardrails to your AI system. Define conditions that trigger actions on incoming logs — run evaluators on live traffic, flag problematic responses, or alert when quality drops below a threshold.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/settings/conditions/create-condition.png"/>
</Frame>

Use automations to:
- **Run online evaluations** — score every response in production automatically
- **Monitor quality metrics** — trigger alerts when evaluation scores drop
- **Flag anomalies** — catch unexpected outputs before users report them

<CardGroup cols={2}>
  <Card title="Automations" icon="bolt" href="/documentation/features/monitoring/automations/quickstart">
    Set up automated monitoring rules
  </Card>
  <Card title="Online evaluation" icon="gauge" href="/documentation/features/monitoring/automations/quickstart_online_eval">
    Run evaluators on live traffic
  </Card>
</CardGroup>
