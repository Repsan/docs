---
title: Vercel AI Gateway
description: Log LLM requests with Respan gateway
og:title: 'Vercel AI SDK with Respan'
---
<Note> This integration is for the **Respan gateway**. </Note>
[Vercel AI SDK](https://ai-sdk.dev/) gateway integration. With this integration, your LLM requests will go through **the Respan gateway**, and the requests will be **automatically logged to Respan**. 


## 1. Get Respan API key

After you create an account on [Respan](https://platform.respan.ai), you can get your API key from the [API keys page](https://platform.respan.ai/platform/api/api-keys).

<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/get-started/api-keys.png" alt="Create API key placeholder" />


## 2. Add provider credentials
You have to add your own credentials to activate AI gateway otherwise your LLM calls will cause errors. We will use your credentials to call LLMs on your behalf.

We **won't use your credentials** for any other purposes and no extra charges will be applied.

Go to the [Providers page](https://platform.respan.ai/platform/api/providers) to add your credentials. 
<Frame>
<img className="block dark:hidden" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/settings/providers.jpg" alt="Providers page"/>
<img className="hidden dark:block" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/settings/providers-dark.jpg" alt="Providers page"/>
</Frame>

<Tip>Learn how to add credentials to a specific provider [here](/documentation/admin/llm_provider_keys).</Tip>

---

## 3. Compatibility at a glance

| SDK helper          | Works via Respan?        | Switch models? |
| ------------------- | ----------------------------- | -------------- |
| `@ai-sdk/openai`    | ✅ Yes                     |  ✅ Yes      |
| `@ai-sdk/anthropic` | ✅ Yes (Anthropic models only) | ❌ No           |
| `@ai-sdk/google`     | ✅ Yes                      | ✅ Yes       |

<small>*Requires overriding `baseURL` and passing the provider key via `customer_credentials`.</small>

---

## 4. Integrate Respan gateway

<Tabs>
<Tab title="OpenAI">
Here's an example of how to use the Vercel AI SDK with OpenAI:
<CodeGroup>
```TypeScript OpenAI.tsx
import { createOpenAI, OpenAIProvider } from '@ai-sdk/openai'
import { streamText, streamObject } from 'ai'

async function main() {
  // Initialize OpenAI with Respan gateways
  const client: OpenAIProvider = createOpenAI({
    baseURL: 'https://api.respan.ai',
    apiKey: 'YOUR_RESPAN_API_KEY',
    compatibility: 'strict',
  })


  const requestParamsDefault: Parameters<typeof streamText>[0] = {
    model: client.chat('gpt-3.5-turbo'),
    messages: [
      {
        role: 'user',
        content: 'Hello! How are you doing today?',
      },
    ],
    temperature: 0.5,
  }

  try {
    console.log('Calling OpenAI with Respan proxy...')

    const { textStream: proxyTextStream } = await streamText(requestParamsDefault)
    for await (const textPart of proxyTextStream) {
      console.log('Respan Proxy Response:', textPart)
    }
  } catch (error) {
    console.error('Error:', error)
  }
}

main()
```
</CodeGroup>

Here is an example of using the Vercel AI SDK with response API.

```typescript TypeScript
import { streamText, streamObject, generateText, tool } from "ai";
import { createOpenAI, openai } from "@ai-sdk/openai";
import { z } from "zod";

// Respan parameters to be sent in the header
const respanHeaderContent = {
    "customer_identifier": "test_customer_identifier_from_header"
}
const encoded = Buffer.from(JSON.stringify(respanHeaderContent)).toString('base64');

const client = createOpenAI({
  headers: {
    "X-Data-Respan-Params": encoded
  },
  baseURL: "https://api.respan.ai/api",
  apiKey: process.env.RESPAN_API_KEY,
});

const result = await generateText({
  model: client.responses('gpt-4o-mini'),
  prompt: 'What happened in San Francisco last week?',
  tools: {
    web_search_preview: client.tools.webSearchPreview({
      searchContextSize: 'high',
      userLocation: {
        type: 'approximate',
        city: 'San Francisco',
        region: 'California',
      },
    }),
  },
});

console.log(result.text);
console.log(result.sources);
```

You can find the full source code for this example on GitHub: [TypeScript Example](https://github.com/Repsan/respan-example-projects/blob/main/example_scripts/typescript/vercel_sdk_example.ts)

</Tab>
<Tab title="Anthropic">
Here's an example of how to use the Vercel AI SDK with Anthropic:
<CodeGroup>
```TypeScript Anthropic.tsx
import { createAnthropic } from '@ai-sdk/anthropic';
import { streamText, convertToCoreMessages } from 'ai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export const anthropic: AnthropicProvider = createAnthropic({
  baseURL: "https://api.respan.ai/api/anthropic/v1",
  apiKey: process.env.RESPAN_API_KEY,
});
   

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = await streamText({
    model: anthropic('claude-3-5-sonnet-20240620'), // choose any model
    messages: convertToCoreMessages(messages),
  });

  return result.toDataStreamResponse();
}
```
</CodeGroup>
</Tab>

<Tab title="Google Gemini">
Here's an example of how to use the Vercel AI SDK with Google Gemini:
```TypeScript Google Gemini.tsx
import {
  createGoogleGenerativeAI,
  GoogleGenerativeAIProviderOptions,
} from "@ai-sdk/google";
import { generateText, streamText } from "ai";


const url = `https://api.respan.ai/api/google/`;

const google = createGoogleGenerativeAI({
  // custom settings
  baseURL: url,
  apiKey: process.env.RESPAN_API_KEY,
  
});

const { text } = await generateText({
  model: google("gemini-2.0-flash"),
  providerOptions: {
    google: {
      responseModalities: ["TEXT", "IMAGE"],
    } satisfies GoogleGenerativeAIProviderOptions,
  },
  prompt: "Hello, how are you?",
  temperature: 0.5,
});
```
</Tab>

</Tabs>

{/* ## Vercel AI SDK Core Functions:

<ParamField path="generateText" type="function">
Generates text and tool calls. This function is ideal for non-interactive use cases such as automation tasks where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.
</ParamField>

<ParamField path="streamText" type="function">
    Stream text and tool calls. You can use the streamText function for interactive use cases such as chat bots and content streaming. You can also generate UI components with tools (see Generative UI).
</ParamField>

<ParamField path="generateObject" type="function">
    Generates a typed, structured object that matches a [Zod](https://zod.dev/) schema. You can use this function to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.
</ParamField> */}

## 5. Add Respan parameters

Adding Respan parameters to the Vercel AI SDK is different than other frameworks. Here is an example of how to do it:

<Steps>
<Step title="Specify Respan params in an object">
You should create an object with the Respan parameters you want to use. Add parameters you want to use as keys in the object.
```typescript
const respanHeaderContent = {
    "customer_params": {
        "customer_identifier": "customer_123",
        "name": "Hendrix Liu", //optional
        "email": "hendrix@respan.ai" //optional
    }
    // "cache_enabled": true or other parameters
}
```
</Step>
<Step title="Encode the object as a string">
You should encode the object as a string and then you can send it as a header in your request.
```typescript
const encoded = Buffer.from(JSON.stringify(respanHeaderContent)).toString('base64');
```
</Step>
<Step title="Add the header to your request">
You should send it in the `X-Data-Respan-Params` header.
<CodeGroup>
```typescript OpenAI
const client = createOpenAI({
  baseURL: process.env.RESPAN_ENDPOINT_LOCAL,
  apiKey: process.env.RESPAN_API_KEY_TEST,
  compatibility: "strict",
  headers: {
    "X-Data-Respan-Params": encoded
  }
});
```
```typescript Anthropic
const client = createAnthropic({
  baseURL: process.env.RESPAN_ENDPOINT_LOCAL,
  apiKey: process.env.RESPAN_API_KEY_TEST,
  headers: {
    "X-Data-Respan-Params": encoded
  }
});
```
</CodeGroup>
</Step>
<Step title="Full example">
<CodeGroup>
```typescript OpenAI
import { streamText, streamObject } from "ai";
import { createOpenAI } from "@ai-sdk/openai";

const respanHeaderContent = {
    "customer_identifier": "test_customer_identifier_from_header"
}
const encoded = Buffer.from(JSON.stringify(respanHeaderContent)).toString('base64');

const client = createOpenAI({
  baseURL: process.env.RESPAN_ENDPOINT_LOCAL,
  apiKey: process.env.RESPAN_API_KEY_TEST,
  compatibility: "strict",
  headers: {
    "X-Data-Respan-Params": encoded
  }
});

const requestParamsDefault: Parameters<typeof streamText>[0] = {
  model: client.chat("gpt-4o"),
  messages: [
    {
      role: "user",
      content: "Hello! How are you doing today?",
    },
  ],
  temperature: 0.5,
};

try {
  console.log("Calling OpenAI with Respan proxy...");

  const { textStream: proxyTextStream } = await streamText(
    requestParamsDefault
  );
  for await (const textPart of proxyTextStream) {
    console.log("Respan Proxy Response:", textPart);
  }
} catch (error) {
  console.error("Error:", error);
}
```
```typescript Anthropic
import { createAnthropic } from '@ai-sdk/anthropic';
import { streamText, convertToCoreMessages } from 'ai';

const respanHeaderContent = {
    "customer_identifier": "test_customer_identifier_from_header"
}
const encoded = Buffer.from(JSON.stringify(respanHeaderContent)).toString('base64');

const anthropic = createAnthropic({
  baseURL: process.env.RESPAN_ENDPOINT_LOCAL,
  apiKey: process.env.RESPAN_API_KEY_TEST,
  headers: {
    "X-Data-Respan-Params": encoded
  }
});
   

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = await streamText({
    model: anthropic('claude-3-5-sonnet-20240620'), // choose any model
    messages: convertToCoreMessages(messages),
  });

  return result.toDataStreamResponse();
}
```
</CodeGroup>
</Step>
</Steps>